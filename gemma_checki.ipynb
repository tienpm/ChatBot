{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login tokenizer\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv()) \n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = HUGGINGFACE_TOKEN)\n",
    "\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time : \n",
      "2024-05-24 18:40:31\n",
      "Current date and time : \n",
      "05_24_18_40\n"
     ]
    }
   ],
   "source": [
    "# get date time \n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print (\"Current date and time : \")\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# get date, moth to string \n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print (\"Current date and time : \")\n",
    "print (now.strftime(\"%m_%d_%H_%M\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not exist\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/xuananh/work/ChatBot/xa_corpus_20_5.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# read file and print 10 first line\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mreadline())\n",
      "File \u001b[0;32m~/anaconda3/envs/code/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xuananh/work/ChatBot/xa_corpus_20_5.txt'"
     ]
    }
   ],
   "source": [
    "path_text = \"/home/xuananh/work/ChatBot/xa_corpus_20_5.txt\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(path_text):\n",
    "    print(\"File exist\")\n",
    "else:\n",
    "    print(\"File not exist\")\n",
    "    \n",
    "\n",
    "# read file and print 10 first line\n",
    "\n",
    "with open(path_text, 'r') as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/xuananh/work/ChatBot/cache/cache_file.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import Path from pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path.cwd() / \"cache\"\n",
    "cache_dir / \"cache_file.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số liệu ngoại hối của Việt Nam được đưa ra gần đây nhất là hồi tháng 6/2011 trong dịp diễn ra Hội nghị tư vấn các nhà tài trợ (CG) giữa kỳ.\n",
      "\n",
      "Khi nào cần giúp đỡ? Nếu bạn thấy máu kinh vón cục quá nhiều và xuất hiện cả những cục máu rất lớn, hãy đến bác sĩ kiểm tra nhé.\n",
      "\n",
      "Trung Đông \"sốt\" biểu tình, có thêm\n",
      "\n",
      "Cụ thể, đảo băng Greenland đã phải đối mặt với tình trạng “nhẹ” đi 240 gigaton trong thời gian từ 2002-2011. Hiện tượng này đã khiến mực nước biển cao thêm tương ứng 0,7mm mỗi năm.\n",
      "\n",
      "Thành viên có tên nangha_3h bày tỏ: “Nhiều người bây giờ chỉ thích nghe những lời giả tạo, hoa mỹ, đó là những chia sẻ thật có gì mà phải thổi phồng lên.\n",
      "\n",
      "Cho đến một ngày, con Mương\n",
      "\n",
      "Chồng chị Thùy sau 2 lần bảo chị mang con về nhà, nhưng chị Thùy không đồng ý thế là bỏ vợ, bỏ con đi biệt tích cho đến nay.\n",
      "\n",
      "Ngay sau đó, người dân gọi cảnh sát PCCC và khẩn\n",
      "\n",
      "Chị tâm sự có con là chuyện riêng của Huy và công ty không có điều khoản cấm.\n",
      "\n",
      "Ở bậc học phổ thông, tâm, sinh lý của học sinh đã có nhiều thay đổi.\n",
      "\n",
      "966810447\n",
      "96563478\n",
      "966810447\n",
      "96563478\n"
     ]
    }
   ],
   "source": [
    "large_text_path = \"xa/xa_corpus_22_1_small.txt\"\n",
    "small_text_path = \"xa/xa_corpus_22_1_super_small.txt\"\n",
    "\n",
    "# get 10% of large text file to store to small text file\n",
    "with open(large_text_path, 'r') as f:\n",
    "    with open(small_text_path, 'w') as f_small:\n",
    "        for i, line in enumerate(f):\n",
    "            if i % 10 == 0:\n",
    "                f_small.write(line)\n",
    "                \n",
    "# read small text file\n",
    "with open(small_text_path, 'r') as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline())\n",
    "        \n",
    "# get size of file\n",
    "import os\n",
    "print(os.path.getsize(large_text_path))\n",
    "print(os.path.getsize(small_text_path))\n",
    "\n",
    "# get size of file\n",
    "from pathlib import Path\n",
    "print(Path(large_text_path).stat().st_size)\n",
    "print(Path(small_text_path).stat().st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "from pathlib import Path\n",
    "import logging\n",
    "xa_folder_path = \"xa\"\n",
    "# xa_folder_path = Path(xa_folder_path)\n",
    "# logging_name_path =  xa_folder_path / \"log_{}.log\".format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "# logging.basicConfig(filename=logging_name_path, level=logging.INFO)\n",
    "# logging.info(\"Start logging\")\n",
    "\n",
    "logging_name_path =  \"xa_tokenizer.log\"\n",
    "# add logging to file path \n",
    "logging.basicConfig(filename=logging_name_path, level=logging.INFO)\n",
    "\n",
    "\n",
    "logging.info(\"Start logging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use input_sentence_size = {input_sentence_size} to train with large corpus\n",
      "do not config input_sentence_size to train with large corpus\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "from pathlib import Path\n",
    "import logging\n",
    "xa_folder_path = \"xa\"\n",
    "xa_folder_path = Path(xa_folder_path)\n",
    "logging_name_path =  \"xa_tokenizer.log\"\n",
    "logging.basicConfig(filename=logging_name_path, level=logging.INFO)\n",
    "logging.info(\"Start logging\")\n",
    "\n",
    "# train sentencepiece model (using sentencepiece library from google) \n",
    "import sentencepiece as spm\n",
    "\n",
    "# use Path of pathlib to cover xa_folder_path to Path object\n",
    "\n",
    "\n",
    "small_text_path = xa_folder_path / \"xa_corpus_22_1_small.txt\"\n",
    "\n",
    "vocab_size = 50000 \n",
    "model_type = \"BPE\"\n",
    "max_sentence_length = 100000 \n",
    "\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     input=small_text_path,\n",
    "#     model_prefix=model_prefix,\n",
    "#     vocab_size=vocab_size\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################### Build tokenizer model from input text file ###################\n",
    "logging.info(\"Start training tokenizer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use input_sentence_size = 1000000 to train with large corpus\n",
    "input_sentence_size = 1000000\n",
    "model_prefix = xa_folder_path / \"xa_use_input_sentence_size\"\n",
    "logging.info(\"use input_sentence_size = {input_sentence_size} to train with large corpus\")\n",
    "print(\"use input_sentence_size = {input_sentence_size} to train with large corpus\")\n",
    "start_time = datetime.datetime.now()\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input = small_text_path,\n",
    "    model_prefix = model_prefix,\n",
    "    shuffle_input_sentence=False,\n",
    "    train_extremely_large_corpus=True,\n",
    "    max_sentence_length = max_sentence_length,\n",
    "    model_type=model_type,\n",
    "    vocab_size=vocab_size,\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    byte_fallback=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    remove_extra_whitespaces=False,\n",
    "    normalization_rule_name=\"nfkc\",\n",
    "    input_sentence_size = input_sentence_size\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "during_time = end_time - start_time\n",
    "# convert all to minutes \n",
    "during_time = during_time.total_seconds() / 60\n",
    "\n",
    "# log time to train tokenizer in miniutes\n",
    "logging.info(f\"Training tokenizer done in  {during_time} minutes with input_sentence_size = {input_sentence_size}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# do not use input_sentence_size to train with large corpus\n",
    "logging.info(\"do not config input_sentence_size to train with large corpus\")\n",
    "print(\"do not config input_sentence_size to train with large corpus\")\n",
    "model_prefix = xa_folder_path / \"xa_not_use_input_sentence_size\"\n",
    "start_time = datetime.datetime.now()\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input = small_text_path,\n",
    "    model_prefix = model_prefix,\n",
    "    shuffle_input_sentence=False,\n",
    "    train_extremely_large_corpus=True,\n",
    "    max_sentence_length = max_sentence_length,\n",
    "    model_type=model_type,\n",
    "    vocab_size=vocab_size,\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    byte_fallback=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    remove_extra_whitespaces=False,\n",
    "    normalization_rule_name=\"nfkc\"\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "during_time = end_time - start_time\n",
    "during_time = during_time.total_seconds() / 60\n",
    "logging.info(f\"Training tokenizer done in {during_time} minutes without input_sentence_size\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running code   \n",
    "\n",
    "- giờ cứ run lại cái đã nhé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  a fox without a tail completing story\n",
      "{'------------------- Customize tokenizer  -------------------'}\n",
      "encode as word piece:  ['▁a', '▁f', 'ox', '▁without', '▁a', '▁tai', 'l', '▁comp', 'let', 'ing', '▁story']\n",
      "len of endoce as word piece:  13\n",
      "{'------------------- Vistral  -------------------'}\n",
      "encode as word piece:  ['▁a', '▁f', 'ox', '▁without', '▁a', '▁tail', '▁comp', 'le', 'ting', '▁story']\n",
      "len of endoce as word piece:  10\n",
      "*********************************************************************************************************************************\n",
      "Sentence:  hôm nay tôi không đi học nên ở nhà nằm trên giường, lung linh, em xinh đẹp mà linh tinhh\n",
      "{'------------------- Customize tokenizer  -------------------'}\n",
      "encode as word piece:  ['▁hôm', '▁nay', '▁tôi', '▁không', '▁đi', '▁học', '▁nên', '▁ở', '▁nhà', '▁nằm', '▁trên', '▁giường', ',', '▁lung', '▁linh', ',', '▁em', '▁xinh', '▁đẹp', '▁mà', '▁linh', '▁tinh', 'h']\n",
      "len of endoce as word piece:  23\n",
      "{'------------------- Vistral  -------------------'}\n",
      "encode as word piece:  ['▁hôm', '▁nay', '▁tôi', '▁không', '▁đi', '▁học', '▁nên', '▁ở', '▁nhà', '▁nằm', '▁trên', '▁giường', ',', '▁lung', '▁linh', ',', '▁em', '▁xinh', '▁đẹp', '▁mà', '▁linh', '▁tinh', 'h']\n",
      "len of endoce as word piece:  23\n",
      "--------------------------------- Qwen1.5-14B-Chat ---------------------------------\n",
      "encode as word piece:  ['h', 'Ã´m', 'Ġn', 'ay', 'ĠtÃ´i', 'ĠkhÃ´ng', 'ĠÄĳi', 'Ġhá»įc', 'ĠnÃªn', 'Ġá»Ł', 'ĠnhÃł', 'Ġnáº±m', 'ĠtrÃªn', 'ĠgiÆ°á»Ŀng', ',', 'Ġlung', 'Ġlin', 'h', ',', 'Ġem', 'Ġx', 'inh', 'ĠÄĳáº¹p', 'ĠmÃł', 'Ġlin', 'h', 'Ġtin', 'hh']\n",
      "len of endoce as word piece:  28\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# load model use input sentence size\n",
    "sp_use_input_sentence_size = spm.SentencePieceProcessor()\n",
    "sp_use_input_sentence_size.load(\"xa/xa_use_input_sentence_size.model\")\n",
    "\n",
    "\n",
    "# load model not use input sentence size\n",
    "sp_not_use_input_sentence_size = spm.SentencePieceProcessor()\n",
    "sp_not_use_input_sentence_size.load(\"xa/xa_not_use_input_sentence_size.model\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# vistral\n",
    "vistral_tokenizer = AutoTokenizer.from_pretrained(\"Viet-Mistral/Vitral-7b-chat\")\n",
    "# Qwen1.5-14B-Chat\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-14B-Chat\")\n",
    "\n",
    "\n",
    "\n",
    "# encode sentence\n",
    "sentence = \"a fox without a tail completing story\" \n",
    "\n",
    "print(\"Sentence: \", sentence)\n",
    "print({\"------------------- Customize tokenizer  -------------------\"})\n",
    "print(\"encode as word piece: \", sp_not_use_input_sentence_size.encode_as_pieces(sentence))\n",
    "print(\"len of endoce as word piece: \", len(sp_use_input_sentence_size.encode_as_pieces(sentence)))\n",
    "\n",
    "\n",
    "print({\"------------------- Vistral  -------------------\"})\n",
    "print(\"encode as word piece: \", vistral_tokenizer.tokenize(sentence))\n",
    "print(\"len of endoce as word piece: \", len(vistral_tokenizer.tokenize(sentence)))\n",
    "\n",
    "\n",
    "print(\"*********************************************************************************************************************************\")\n",
    "sentence = \"hôm nay tôi không đi học nên ở nhà nằm trên giường, lung linh, em xinh đẹp mà linh tinhh\"\n",
    "\n",
    "print(\"Sentence: \", sentence)\n",
    "print({\"------------------- Customize tokenizer  -------------------\"})\n",
    "print(\"encode as word piece: \", sp_not_use_input_sentence_size.encode_as_pieces(sentence))\n",
    "print(\"len of endoce as word piece: \", len(sp_use_input_sentence_size.encode_as_pieces(sentence)))\n",
    "\n",
    "\n",
    "print({\"------------------- Vistral  -------------------\"})\n",
    "print(\"encode as word piece: \", vistral_tokenizer.tokenize(sentence))\n",
    "print(\"len of endoce as word piece: \", len(vistral_tokenizer.tokenize(sentence)))\n",
    "\n",
    "print(\"--------------------------------- Qwen1.5-14B-Chat ---------------------------------\")\n",
    "print(\"encode as word piece: \", qwen_tokenizer.tokenize(sentence))\n",
    "print(\"len of endoce as word piece: \", len(qwen_tokenizer.tokenize(sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train with Tokenizers library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentencepiece from tokenizations library hugging facce \n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "text_path = \"xa/xa_corpus_22_1.txt\"\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer_trainer = SentencePieceBPETokenizer()\n",
    "tokenizer_trainer.train(files=[str(text_path)] )\n",
    "tokenizer_trainer.save(\"xa/xa_tokenizer_tokenizers\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"xa/xa_tokenizer_tokenizers\")\n",
    "\n",
    "# encode sentence\n",
    "sentence = \"hôm nay tôi không đi học nên ở nhà nằm trên giường, lung linh, em xinh đẹp mà linh tinhh\"\n",
    "print(\"Sentence: \", sentence)\n",
    "print({\"------------------- Customize tokenizer  -------------------\"})\n",
    "print(\"encode as word piece: \", tokenizer.encode(sentence).tokens)\n",
    "print(\"len of endoce as word piece: \", len(tokenizer.encode(sentence).tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train with tokenizer library from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_tokenizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mpre_tokenizers\u001b[49m\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_tokenizers' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef73360e96f8472cbf0bf07321857622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tokenizers\n",
    "# load dataset from text path \n",
    "text_path = \"xa/xa_corpus_22_1.txt\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_files=[text_path], name = [\"wikitext-2-raw-v1\", \"tasksource/bigbench\"], split = \"train\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 4000):\n",
    "        yield dataset[i : i + 4000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=50000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer\n",
    "tokenizer.save(\"xa/xa_tokenizer_tokenizers_scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'------------------- Customize tokenizer  -------------------'}\n",
      "encode as word piece:  ['hÃ´m', 'Ġnay', 'ĠtÃ´i', 'ĠkhÃ´ng', 'ĠÄĳi', 'Ġhá»įc', 'ĠnÃªn', 'Ġá»Ł', 'ĠnhÃł', 'Ġnáº±m', 'ĠtrÃªn', 'ĠgiÆ°á»Ŀng', ',', 'Ġlung', 'Ġlinh', ',', 'Ġem', 'Ġxinh', 'ĠÄĳáº¹p', 'ĠmÃł', 'Ġlinh', 'Ġtinh', 'h']\n",
      "len of endoce as word piece:  23\n",
      "Sentence:  a fox without a tail completing story\n",
      "{'------------------- Customize tokenizer  -------------------'}\n",
      "encode as word piece:  ['a', 'Ġf', 'ox', 'Ġwithout', 'Ġa', 'Ġtai', 'l', 'Ġcomp', 'let', 'ing', 'Ġstory']\n",
      "len of endoce as word piece:  11\n"
     ]
    }
   ],
   "source": [
    "print({\"------------------- Customize tokenizer  -------------------\"})\n",
    "sentence = \"hôm nay tôi không đi học nên ở nhà nằm trên giường, lung linh, em xinh đẹp mà linh tinhh\"\n",
    "print(\"encode as word piece: \", tokenizer.encode(sentence).tokens)\n",
    "print(\"len of endoce as word piece: \", len(tokenizer.encode(sentence).tokens))\n",
    "\n",
    "sentence = \"a fox without a tail completing story\"\n",
    "print(\"Sentence: \", sentence)\n",
    "print({\"------------------- Customize tokenizer  -------------------\"})\n",
    "print(\"encode as word piece: \", tokenizer.encode(sentence).tokens)\n",
    "print(\"len of endoce as word piece: \", len(tokenizer.encode(sentence).tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
